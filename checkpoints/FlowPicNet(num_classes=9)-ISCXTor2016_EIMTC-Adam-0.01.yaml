batch_size: 256
dataset: ISCXTor2016_EIMTC
epochs: 40
info_batch: 10
loss_func: nn.CrossEntropyLoss()
num_classes: 9
model: FlowPicNet(num_classes=9)
lr: 0.001
# 注意optim的eval依赖model
optim: optim.Adam(params=self.config['model'].parameters(), lr=self.config['lr'],betas=[0.9, 0.999])

# with softmax
#0.00932548968876056
#0.00916319795741253
#0.00916319795741253
#0.00916319795741253
#0.00916319795741253
#0.00916319795741253
#0.00916319795741253
#0.00916319795741253
#0.00916319795741253
#0.00916319795741253
#0.00916319795741253
#0.00916319795741253
#0.00916319795741253
#0.00916319795741253
#0.00916319795741253
#0.00916319795741253
#0.00916319795741253
#0.00916319795741253
#0.00916319795741253

# without softmax
#0.010683706274932628
#0.008796762533706069
#0.008677197275443572
#0.008903129766735154
#0.008489437530561215
#0.008367312147687798
#0.008239810346307018
#0.008318321993512352
#0.008276085426286928
#0.008243816210271291

# without softmax, lr=0.01，较上方增加了一些输出
#train_loss: 0.010683706274932628, val_loss: 0.010141466744244099, val_acc: 0.1512158066034317
#epoch: 1
#train_loss: 0.008796762533706069, val_loss: 0.009960519149899483, val_acc: 0.1512158066034317
#epoch: 2
#train_loss: 0.008677197275443572, val_loss: 0.009800562635064125, val_acc: 0.1512158066034317
#epoch: 3
#train_loss: 0.008903129766735154, val_loss: 0.009663558565080166, val_acc: 0.1512158066034317
#epoch: 4
#train_loss: 0.008489437530561215, val_loss: 0.009492987766861916, val_acc: 0.15805470943450928
#epoch: 5
#train_loss: 0.008367312102231444, val_loss: 0.009247138164937496, val_acc: 0.2598784267902374
#epoch: 6
#train_loss: 0.008239810323578841, val_loss: 0.009359722957015038, val_acc: 0.22948327660560608
#epoch: 7
#train_loss: 0.008318321993512352, val_loss: 0.00928709376603365, val_acc: 0.22948327660560608
#epoch: 8
#train_loss: 0.008276085426286928, val_loss: 0.009225471876561642, val_acc: 0.22948327660560608
#epoch: 9
#train_loss: 0.008243816210271291, val_loss: 0.009172690100967884, val_acc: 0.22948327660560608
#epoch: 10
#train_loss: 0.008217118556892678, val_loss: 0.009127309545874596, val_acc: 0.22948327660560608
#epoch: 11
#train_loss: 0.008195056610725629, val_loss: 0.009088226594030857, val_acc: 0.22948327660560608
#epoch: 12
#train_loss: 0.008176834794654518, val_loss: 0.009054522961378098, val_acc: 0.22948327660560608
#epoch: 13
#train_loss: 0.008161790718682274, val_loss: 0.009025421924889088, val_acc: 0.22948327660560608
#epoch: 14
#train_loss: 0.008149373929084654, val_loss: 0.009000261314213276, val_acc: 0.22948327660560608
#epoch: 15
#train_loss: 0.008139128089519315, val_loss: 0.008978476747870445, val_acc: 0.22948327660560608
#epoch: 16
#train_loss: 0.008130674775835442, val_loss: 0.008959590457379818, val_acc: 0.22948327660560608
#epoch: 17
#train_loss: 0.00812370063465362, val_loss: 0.008943195454776287, val_acc: 0.22948327660560608
#epoch: 18
#train_loss: 0.008117945860179977, val_loss: 0.008928940631449223, val_acc: 0.22948327660560608
#epoch: 19
#train_loss: 0.008113196557538638, val_loss: 0.008916528895497322, val_acc: 0.22948327660560608
#epoch: 20
#train_loss: 0.008109275674229014, val_loss: 0.008905709721148014, val_acc: 0.22948327660560608
#epoch: 21
#train_loss: 0.00810603738626602, val_loss: 0.008896260522305965, val_acc: 0.22948327660560608
#epoch: 22
#train_loss: 0.008103361188854004, val_loss: 0.008887997828423977, val_acc: 0.22948327660560608
#epoch: 23
#train_loss: 0.008101148509888335, val_loss: 0.008880763314664364, val_acc: 0.22948327660560608
#epoch: 24
#train_loss: 0.00809931755065918, val_loss: 0.008874418213963509, val_acc: 0.22948327660560608
#epoch: 25
#train_loss: 0.0080978015812382, val_loss: 0.008868847973644733, val_acc: 0.22948327660560608
#epoch: 26
#train_loss: 0.008096545144872966, val_loss: 0.008863949216902256, val_acc: 0.22948327660560608
#epoch: 27
#train_loss: 0.008095503671607935, val_loss: 0.008859637193381786, val_acc: 0.22948327660560608
#epoch: 28
#train_loss: 0.008094638978105369, val_loss: 0.008855834603309631, val_acc: 0.22948327660560608
#epoch: 29
#train_loss: 0.008093920540423252, val_loss: 0.008852479979395866, val_acc: 0.22948327660560608
#epoch: 30
#train_loss: 0.008093323584848409, val_loss: 0.00884951464831829, val_acc: 0.22948327660560608
#epoch: 31
#train_loss: 0.008092826655981539, val_loss: 0.008846892043948174, val_acc: 0.22948327660560608
#epoch: 32
#train_loss: 0.00809241311679691, val_loss: 0.008844567462801933, val_acc: 0.22948327660560608
#epoch: 33
#train_loss: 0.008092068762183758, val_loss: 0.008842507377266884, val_acc: 0.22948327660560608
#epoch: 34
#train_loss: 0.008091781137100962, val_loss: 0.008840679191052914, val_acc: 0.22948327660560608
#epoch: 35
#train_loss: 0.00809154180939477, val_loss: 0.008839054964482784, val_acc: 0.22948327660560608
#epoch: 36
#train_loss: 0.008091341687794296, val_loss: 0.00883761141449213, val_acc: 0.22948327660560608
#epoch: 37
#train_loss: 0.008091174953886164, val_loss: 0.008836324326694012, val_acc: 0.22948327660560608
#epoch: 38
#train_loss: 0.008091035811985187, val_loss: 0.008835176937282085, val_acc: 0.22948327660560608
#epoch: 39
#train_loss: 0.008090919398261434, val_loss: 0.008834156207740307, val_acc: 0.22948327660560608

#lr =0.001
#epoch: 0
#D:\projects\internet_flow\FlowPic\trainer.py:103: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
#  pre = nn.Softmax()(out).argmax(dim=1)
#train_loss: 2.3134842770440236, val_loss: 2.219221591949463, val_acc: 0.1512158066034317
#epoch: 1
#train_loss: 2.2090791861216226, val_loss: 2.2130885124206543, val_acc: 0.1512158066034317
#epoch: 2
#train_loss: 2.2030232747395835, val_loss: 2.201798915863037, val_acc: 0.15805470943450928
#epoch: 3
#train_loss: 2.190246479851859, val_loss: 2.172415256500244, val_acc: 0.2036474198102951
#epoch: 4
#train_loss: 2.155557076136271, val_loss: 2.1193699836730957, val_acc: 0.21884498000144958
#epoch: 5
#train_loss: 2.0902130206425986, val_loss: 1.966065764427185, val_acc: 0.3191489279270172
#epoch: 6
#train_loss: 2.005634245418367, val_loss: 1.8201701641082764, val_acc: 0.40273556113243103
#epoch: 7
#train_loss: 1.7719119162786574, val_loss: 1.500864863395691, val_acc: 0.5136778354644775
#epoch: 8
#train_loss: 1.7382837619100298, val_loss: 1.4174503087997437, val_acc: 0.5379939079284668
#epoch: 9
#train_loss: 1.5450619232086908, val_loss: 1.2297406196594238, val_acc: 0.5417932868003845
#epoch: 10
#train_loss: 1.4538844369706654, val_loss: 1.153270959854126, val_acc: 0.5402735471725464
#epoch: 11
#train_loss: 1.3673243948391505, val_loss: 1.06575608253479, val_acc: 0.552431583404541
#epoch: 12
#train_loss: 1.3083259108520688, val_loss: 1.0198934078216553, val_acc: 0.5516717433929443
#epoch: 13
#train_loss: 1.2659388752210707, val_loss: 0.9760791659355164, val_acc: 0.5653495192527771
#epoch: 14
#train_loss: 1.2366680957022167, val_loss: 0.9453169107437134, val_acc: 0.5835866332054138
#epoch: 15
#train_loss: 1.1870548810277666, val_loss: 0.910108208656311, val_acc: 0.5927051901817322
#epoch: 16
#train_loss: 1.1501218257915407, val_loss: 0.8930648565292358, val_acc: 0.5949848294258118
#epoch: 17
#train_loss: 1.1187509816317331, val_loss: 0.8476213216781616, val_acc: 0.6109422445297241
#epoch: 18
#train_loss: 1.0790526391494841, val_loss: 0.8389917612075806, val_acc: 0.6162614226341248
#epoch: 19
#train_loss: 1.0493576122181756, val_loss: 0.8071668148040771, val_acc: 0.630699098110199
#epoch: 20
#train_loss: 1.0127076038292475, val_loss: 0.7903989553451538, val_acc: 0.6238601803779602
#epoch: 21
#train_loss: 0.9745237465415683, val_loss: 0.7768230438232422, val_acc: 0.6322188377380371
#epoch: 22
#train_loss: 0.966569184547379, val_loss: 0.7414893507957458, val_acc: 0.6291793584823608
#epoch: 23
#train_loss: 0.9606150097790218, val_loss: 0.7207516431808472, val_acc: 0.6352583765983582
#epoch: 24
#train_loss: 0.9079724975994655, val_loss: 0.71147620677948, val_acc: 0.6436170339584351
#epoch: 25
#train_loss: 0.9027739387182963, val_loss: 0.7009389400482178, val_acc: 0.6428571343421936
#epoch: 26
#train_loss: 0.8672593686552275, val_loss: 0.682715892791748, val_acc: 0.6595744490623474
#epoch: 27
#train_loss: 0.8552315497682208, val_loss: 0.7045876979827881, val_acc: 0.6504559516906738
#epoch: 28
#train_loss: 0.8589264729193279, val_loss: 0.669139564037323, val_acc: 0.6489361524581909
#epoch: 29
#train_loss: 0.8263457665840784, val_loss: 0.7083843946456909, val_acc: 0.6588146090507507
#epoch: 30
#train_loss: 0.8435132418360028, val_loss: 0.6504707336425781, val_acc: 0.6610942482948303
#epoch: 31
#train_loss: 0.7888111670811971, val_loss: 0.6517731547355652, val_acc: 0.6610942482948303
#epoch: 32
#train_loss: 0.7756842176119486, val_loss: 0.6419785022735596, val_acc: 0.6603343486785889
#epoch: 33
#train_loss: 0.7652755000051998, val_loss: 0.6427077054977417, val_acc: 0.6641337275505066
#epoch: 34
#train_loss: 0.7503371403685638, val_loss: 0.6320613622665405, val_acc: 0.6686930060386658
#epoch: 35
#train_loss: 0.7452400617656254, val_loss: 0.6481270790100098, val_acc: 0.6610942482948303
#epoch: 36
#train_loss: 0.7350573401365962, val_loss: 0.624683678150177, val_acc: 0.6633738875389099
#epoch: 37
#train_loss: 0.7115145205032258, val_loss: 0.6265133619308472, val_acc: 0.6626139879226685
#epoch: 38
#train_loss: 0.7128533181690034, val_loss: 0.6267905831336975, val_acc: 0.6610942482948303
#epoch: 39
#train_loss: 0.6980571920673052, val_loss: 0.626588761806488, val_acc: 0.6664133667945862